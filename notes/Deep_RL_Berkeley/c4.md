# [@](./README.md) Lecture 4: Introduction to Reinforcement Learning

**Today**: high-level overview of RL methods.

1. Definition of a MDP
2. Definition of RL problem
3. Anatomy of a RL algorithm
4. Brief overview of RL algorithm types




## 1. Definitions

### Terminology and notation

![](fig0.png)

**Remark**: some RL methods do handle partial observability, some others do not.

**Remark**: you cannot just take the greedy action wrt to the reward, you have to think about what will be the rewards in the future.

### Markov chain

- <img src="/notes/Deep_RL_Berkeley/tex/cef39aeb23a61b09d838693a0897fe03.svg?invert_in_darkmode&sanitize=true" align=middle width=11.187179849999989pt height=22.465723500000017pt/> - state space
- <img src="/notes/Deep_RL_Berkeley/tex/6937e14ec122765a9d014f2cbcf4fcfe.svg?invert_in_darkmode&sanitize=true" align=middle width=13.13115539999999pt height=22.465723500000017pt/> - transition operator

>Transition "operator" since it is a linear operator that apply to vector of probabilities <img src="/notes/Deep_RL_Berkeley/tex/07617f9d8fe48b4a7b3f523d6730eef0.svg?invert_in_darkmode&sanitize=true" align=middle width=9.90492359999999pt height=14.15524440000002pt/>: 
>
>- <img src="/notes/Deep_RL_Berkeley/tex/424feb7e620c37597c29f6ba8ffbbffd.svg?invert_in_darkmode&sanitize=true" align=middle width=108.2952552pt height=24.65753399999998pt/> - <img src="/notes/Deep_RL_Berkeley/tex/87eefe082e181864d1321025c2705ecd.svg?invert_in_darkmode&sanitize=true" align=middle width=14.870715749999988pt height=14.15524440000002pt/> is a vector of probabilites, summing to 1.
>- <img src="/notes/Deep_RL_Berkeley/tex/3df42b5aac1025b58b0873a40785f563.svg?invert_in_darkmode&sanitize=true" align=middle width=172.81345455pt height=24.65753399999998pt/>
>
>Then <img src="/notes/Deep_RL_Berkeley/tex/5f0babf61e2cdf4e51a11061db885476.svg?invert_in_darkmode&sanitize=true" align=middle width=82.2560178pt height=22.465723500000017pt/>.

Markov property: <img src="/notes/Deep_RL_Berkeley/tex/1fe81d9e29c7f0912a19b290850df5fd.svg?invert_in_darkmode&sanitize=true" align=middle width=69.2524701pt height=24.65753399999998pt/> is independent of <img src="/notes/Deep_RL_Berkeley/tex/7f55d81dc342c12cbe8394d0b8f0f0f6.svg?invert_in_darkmode&sanitize=true" align=middle width=29.49784034999999pt height=14.15524440000002pt/>.

![](fig1.png)


### Markov Decision Process

MDP: <img src="/notes/Deep_RL_Berkeley/tex/a3c76d5be185a13d94ae41bf7187a903.svg?invert_in_darkmode&sanitize=true" align=middle width=125.33134514999998pt height=24.65753399999998pt/> 

with:

- <img src="/notes/Deep_RL_Berkeley/tex/cef39aeb23a61b09d838693a0897fe03.svg?invert_in_darkmode&sanitize=true" align=middle width=11.187179849999989pt height=22.465723500000017pt/> - state space
- <img src="/notes/Deep_RL_Berkeley/tex/7651ba0e8e29ee7537841a819041a172.svg?invert_in_darkmode&sanitize=true" align=middle width=13.12555859999999pt height=22.465723500000017pt/> - action space
- <img src="/notes/Deep_RL_Berkeley/tex/6937e14ec122765a9d014f2cbcf4fcfe.svg?invert_in_darkmode&sanitize=true" align=middle width=13.13115539999999pt height=22.465723500000017pt/> - transition operator (now a tensor)

>- <img src="/notes/Deep_RL_Berkeley/tex/94a804e4dc13b2761c8fc3f1deca5485.svg?invert_in_darkmode&sanitize=true" align=middle width=111.7960536pt height=24.65753399999998pt/> 
>- <img src="/notes/Deep_RL_Berkeley/tex/b0ddedddbed2767221acb5b728725c21.svg?invert_in_darkmode&sanitize=true" align=middle width=112.59310095pt height=24.65753399999998pt/> 
>- <img src="/notes/Deep_RL_Berkeley/tex/7d59c94dee775efc843e03b68e1775a9.svg?invert_in_darkmode&sanitize=true" align=middle width=235.138299pt height=24.65753399999998pt/>
>
>Then <img src="/notes/Deep_RL_Berkeley/tex/adeddd985cee909cbe2112caed614a3b.svg?invert_in_darkmode&sanitize=true" align=middle width=184.21502055pt height=24.657735299999988pt/>.

- <img src="/notes/Deep_RL_Berkeley/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode&sanitize=true" align=middle width=7.87295519999999pt height=14.15524440000002pt/> - reward function <img src="/notes/Deep_RL_Berkeley/tex/9c5aa7da813ee2293be60f81d4216133.svg?invert_in_darkmode&sanitize=true" align=middle width=55.934280599999994pt height=24.65753399999998pt/>.

![](fig2.png)


### Partially Observed MDP

POMDP: <img src="/notes/Deep_RL_Berkeley/tex/f666f8a9301097be9af933cfdb79ad77.svg?invert_in_darkmode&sanitize=true" align=middle width=163.63267304999997pt height=24.65753399999998pt/> 

with:

- <img src="/notes/Deep_RL_Berkeley/tex/cef39aeb23a61b09d838693a0897fe03.svg?invert_in_darkmode&sanitize=true" align=middle width=11.187179849999989pt height=22.465723500000017pt/> - state space
- <img src="/notes/Deep_RL_Berkeley/tex/7651ba0e8e29ee7537841a819041a172.svg?invert_in_darkmode&sanitize=true" align=middle width=13.12555859999999pt height=22.465723500000017pt/> - action space
- <img src="/notes/Deep_RL_Berkeley/tex/9fa4bf66c871f8af69c9d3cf2fcb6a55.svg?invert_in_darkmode&sanitize=true" align=middle width=13.54343924999999pt height=22.465723500000017pt/> - observation space
- <img src="/notes/Deep_RL_Berkeley/tex/6937e14ec122765a9d014f2cbcf4fcfe.svg?invert_in_darkmode&sanitize=true" align=middle width=13.13115539999999pt height=22.465723500000017pt/> - transition operator (like before)
- <img src="/notes/Deep_RL_Berkeley/tex/7114e8b70a29f3808a4b0ac1fc360fba.svg?invert_in_darkmode&sanitize=true" align=middle width=10.146128849999991pt height=22.465723500000017pt/> - emission probability <img src="/notes/Deep_RL_Berkeley/tex/dd39fb2f86b24fdcb417bc5ca23f6de1.svg?invert_in_darkmode&sanitize=true" align=middle width=52.87112984999999pt height=24.65753399999998pt/>.
- <img src="/notes/Deep_RL_Berkeley/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode&sanitize=true" align=middle width=7.87295519999999pt height=14.15524440000002pt/> - reward function.

![](fig3.png)

Note that observations are independent given the states.


## 2. The reinforcement learning problem

### The goal of reinforcement learning

Produce a policy <img src="/notes/Deep_RL_Berkeley/tex/a5ed44ffa9d1a35c3459d1a16cf6ce46.svg?invert_in_darkmode&sanitize=true" align=middle width=50.553770849999985pt height=24.65753399999998pt/> (let's come back to POMDP later).

Probability of a trajectory, using the graphical model: 

<img src="/notes/Deep_RL_Berkeley/tex/6bb61a2002d79ca41993d87944a5cad9.svg?invert_in_darkmode&sanitize=true" align=middle width=453.77636535pt height=32.256008400000006pt/>

Optimization problem (expected reward): 

<img src="/notes/Deep_RL_Berkeley/tex/7836a10fbb2ea21582f19213ae243cfb.svg?invert_in_darkmode&sanitize=true" align=middle width=263.077221pt height=27.94539330000001pt/>


### MDP to Markov chain

Given a policy <img src="/notes/Deep_RL_Berkeley/tex/5d757cc357b6324cab87dfb959c6cc26.svg?invert_in_darkmode&sanitize=true" align=middle width=15.98560754999999pt height=14.15524440000002pt/>, we can turn our MDP into a Markov *chain* by grouping states and actions together:

<img src="/notes/Deep_RL_Berkeley/tex/737f737a2e394d3277337efd597675fb.svg?invert_in_darkmode&sanitize=true" align=middle width=244.11508274999997pt height=24.65753399999998pt/>

The transition probability of this Markov chain is:

<img src="/notes/Deep_RL_Berkeley/tex/0961ecd128c8383cba43bf5e8da9cf35.svg?invert_in_darkmode&sanitize=true" align=middle width=365.6856357pt height=27.94539330000001pt/>

We can write our optimization objective using the *state-action* marginal <img src="/notes/Deep_RL_Berkeley/tex/f041a31a5de8f831c7e016efae42951d.svg?invert_in_darkmode&sanitize=true" align=middle width=63.76905974999999pt height=24.65753399999998pt/>:

<img src="/notes/Deep_RL_Berkeley/tex/b765db057f8a072dc4248d95007f7058.svg?invert_in_darkmode&sanitize=true" align=middle width=328.76791365pt height=32.256008400000006pt/>


### Infinite horizon: stationary distribution thanks to ergodicity

We divide our optimization objective by <img src="/notes/Deep_RL_Berkeley/tex/2f118ee06d05f3c2d98361d9c30e38ce.svg?invert_in_darkmode&sanitize=true" align=middle width=11.889314249999991pt height=22.465723500000017pt/>, so as we may obtain something *finite* when <img src="/notes/Deep_RL_Berkeley/tex/2f118ee06d05f3c2d98361d9c30e38ce.svg?invert_in_darkmode&sanitize=true" align=middle width=11.889314249999991pt height=22.465723500000017pt/> goes to infinity: 

<img src="/notes/Deep_RL_Berkeley/tex/5f4e5772bb72e746e61f3faf7c82375b.svg?invert_in_darkmode&sanitize=true" align=middle width=344.98644344999997pt height=32.256008400000006pt/>

Under **ergodicity** assumption (roughly speaking, every <img src="/notes/Deep_RL_Berkeley/tex/29f33abd86efd1f9dd07f2a18990a290.svg?invert_in_darkmode&sanitize=true" align=middle width=23.700515849999988pt height=14.15524440000002pt/> is reachable from every <img src="/notes/Deep_RL_Berkeley/tex/29f33abd86efd1f9dd07f2a18990a290.svg?invert_in_darkmode&sanitize=true" align=middle width=23.700515849999988pt height=14.15524440000002pt/> with non-zero probability),
 <img src="/notes/Deep_RL_Berkeley/tex/150f40a4dbf1e4ff2d5b38534d926e6c.svg?invert_in_darkmode&sanitize=true" align=middle width=56.331892649999986pt height=24.65753399999998pt/> converges to a *stationary* distribution <img src="/notes/Deep_RL_Berkeley/tex/8f7cebe77218ad4b4f6fd38034200ce4.svg?invert_in_darkmode&sanitize=true" align=middle width=44.75651729999999pt height=24.65753399999998pt/>.
 
This means that our optimization problem becomes, in the limite <img src="/notes/Deep_RL_Berkeley/tex/fd7c45eb2515ec78286f8d1b68e23a10.svg?invert_in_darkmode&sanitize=true" align=middle width=53.89831424999999pt height=22.465723500000017pt/>:

<img src="/notes/Deep_RL_Berkeley/tex/eb8342bb1ee93ab3f30a282623dda0c2.svg?invert_in_darkmode&sanitize=true" align=middle width=252.85058039999998pt height=27.94539330000001pt/>


### Expectations

In RL, we often we deal with **expected** reward. 

In particular, even if the reward is not smooth, its expectation with respect to the trajectories of a policy can be smooth in <img src="/notes/Deep_RL_Berkeley/tex/27e556cf3caa0673ac49a8f0de3c73ca.svg?invert_in_darkmode&sanitize=true" align=middle width=8.17352744999999pt height=22.831056599999986pt/>. 

Note that we can explicit our expecations over trajectories (using Markov property):

<img src="/notes/Deep_RL_Berkeley/tex/b23ffb9eb312548f6df9766f332dfce1.svg?invert_in_darkmode&sanitize=true" align=middle width=725.4329313pt height=83.8364835pt/> 


## 3. Anatomy of a reinforcement learning algorithm

![](fig4.png)

Each block can be trivial or expensive, depending of the situation.


### Q-function 

Q for quality. 

<img src="/notes/Deep_RL_Berkeley/tex/03999bce02cab02f81cbc95258c67894.svg?invert_in_darkmode&sanitize=true" align=middle width=69.97862684999998pt height=24.65753399999998pt/> is the total expected reward from taking <img src="/notes/Deep_RL_Berkeley/tex/9789555e5d8fa5de21171cc40c86d2cd.svg?invert_in_darkmode&sanitize=true" align=middle width=13.65494624999999pt height=14.15524440000002pt/> in <img src="/notes/Deep_RL_Berkeley/tex/1f1c28e0a1b1708c6889fb006c886784.svg?invert_in_darkmode&sanitize=true" align=middle width=12.67127234999999pt height=14.15524440000002pt/>.

<img src="/notes/Deep_RL_Berkeley/tex/55ce122ea815060fe6317788099ef763.svg?invert_in_darkmode&sanitize=true" align=middle width=281.78699174999997pt height=32.256008400000006pt/>

In RL, we will try to estimate this quantity.

Value function: <img src="/notes/Deep_RL_Berkeley/tex/553cf5f370e894969ae451cb12130755.svg?invert_in_darkmode&sanitize=true" align=middle width=241.45936979999993pt height=27.94539330000001pt/>


### Using Q function 

Idea 1 (q-learning like): if we have a policy <img src="/notes/Deep_RL_Berkeley/tex/f30fdded685c83b0e7b446aa9c9aa120.svg?invert_in_darkmode&sanitize=true" align=middle width=9.96010619999999pt height=14.15524440000002pt/> and we know <img src="/notes/Deep_RL_Berkeley/tex/b493d4ea8dfad9b043732f527559c9bf.svg?invert_in_darkmode&sanitize=true" align=middle width=58.40325149999999pt height=24.65753399999998pt/>, then we can *improve* <img src="/notes/Deep_RL_Berkeley/tex/f30fdded685c83b0e7b446aa9c9aa120.svg?invert_in_darkmode&sanitize=true" align=middle width=9.96010619999999pt height=14.15524440000002pt/>, by choosing actions with high q-value. 

Idea 2 (actor-critic like): compute gradient wrt <img src="/notes/Deep_RL_Berkeley/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode&sanitize=true" align=middle width=8.68915409999999pt height=14.15524440000002pt/> to increase probability of good actions <img src="/notes/Deep_RL_Berkeley/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode&sanitize=true" align=middle width=8.68915409999999pt height=14.15524440000002pt/>. A good action is such that <img src="/notes/Deep_RL_Berkeley/tex/24ece94bc82d396cbd3337d9041986be.svg?invert_in_darkmode&sanitize=true" align=middle width=101.05807139999999pt height=24.65753399999998pt/>.


### Types of RL algorithms

+ Policy gradients: directly differentiate wrt <img src="/notes/Deep_RL_Berkeley/tex/27e556cf3caa0673ac49a8f0de3c73ca.svg?invert_in_darkmode&sanitize=true" align=middle width=8.17352744999999pt height=22.831056599999986pt/> the objective (expected total reward).
+ Value-based: estimate V or Q of the optimal policy (no explicit policy).
+ Actor-critic: estimate V or Q of the current policy, use it to improve the policy (kind of combination of the first two).
+ Model-based RL: estimate the transition model...
	- Use it for planning
	- Use it to improve a policy
	- Something else
	
	
### Tradeoffs (why so many RL algorithms?)

- Different tradeoffs
	- Sample efficiency (off policy?)
	- Stability and ease of use 
- Different assumptions 
	- Full observability?
	- Stochastic or deterministic?
	- Continuous or discrete?
	- Episodic or infinite horizon?
- Different things are easy or hard in different settings
	- Easier to represent the policy? 
	- Easier to represent the model? 

	
### Examples

![](fig5.png)
