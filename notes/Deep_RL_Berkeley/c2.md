# [@](./README.md) Lecture 2: Supervised Learning of Behaviors

**Today**: 
1. define sequential decision problems
2. Imitation learning: superived learning for decision making
3. A bit of theory
4. Case studies of recent deep imitation learning work


## Terminology and Notations

<img src="/notes/Deep_RL_Berkeley/tex/fb55787c4bf6a2f635dc4fb7aed05601.svg?invert_in_darkmode&sanitize=true" align=middle width=71.52391454999999pt height=24.65753399999998pt/>

Where: 
- <img src="/notes/Deep_RL_Berkeley/tex/23726ac68747e4a9a12c68f294f8dacf.svg?invert_in_darkmode&sanitize=true" align=middle width=12.933843449999989pt height=14.15524440000002pt/> the observation (ex: an image of tiger)
- <img src="/notes/Deep_RL_Berkeley/tex/9789555e5d8fa5de21171cc40c86d2cd.svg?invert_in_darkmode&sanitize=true" align=middle width=13.65494624999999pt height=14.15524440000002pt/> the action (ex: labelling 'tiger', or run away, or a continuous action)
- <img src="/notes/Deep_RL_Berkeley/tex/5d757cc357b6324cab87dfb959c6cc26.svg?invert_in_darkmode&sanitize=true" align=middle width=15.98560754999999pt height=14.15524440000002pt/> the *policy*. 

**Remark**: in this setting, we miss the fact that the action you take affects the next observation you see.

**State vs. Observation**

The state fully describes what happens in the world, whereas the observation is what is "available":
- <img src="/notes/Deep_RL_Berkeley/tex/6b37ef4eb4cda4e896f00a86e60ec891.svg?invert_in_darkmode&sanitize=true" align=middle width=71.26134344999998pt height=24.65753399999998pt/> = fully-observed situation
- <img src="/notes/Deep_RL_Berkeley/tex/fb55787c4bf6a2f635dc4fb7aed05601.svg?invert_in_darkmode&sanitize=true" align=middle width=71.52391454999999pt height=24.65753399999998pt/> = partially-observed situation

Markov property: future independent of the past given the present. WARNING: valid for states, not observations! (ex: car hiding the cheetah) 


## Behavioral cloning

**Principle**: Supervised learning on pairs <img src="/notes/Deep_RL_Berkeley/tex/6bc0cd41196f524772df8fbd5ad8c2a1.svg?invert_in_darkmode&sanitize=true" align=middle width=48.323898149999984pt height=24.65753399999998pt/> given by human demonstrations.

**Butterfly effect**: Small mistakes become huge mistakes because during the trajectory, because small errors compound. Small generalization error at first puts you in a unknown situation, and you make a bigger generalization error, and so on.

**Improvements**: 
- Data augmentation, for stability: addtional cameras, ask human to make small errors, etc.
- DAgger (Dataset Aggregation): add on-policy data, labeled by human expert, then retrain, etc.

Problems with trying to directly fit the expert: 
1. Non-markovian behavior: human demonstrators are not markovian (<img src="/notes/Deep_RL_Berkeley/tex/e2970fb6f5d39fb269ea9458fb4e9bc0.svg?invert_in_darkmode&sanitize=true" align=middle width=108.32947124999998pt height=24.65753399999998pt/>). We may want to include history, but beware of *causal confusion* (brake light) in IL.
2. Multimodal behavior: a solution is to output a mixture of gaussians (but up to <img src="/notes/Deep_RL_Berkeley/tex/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode&sanitize=true" align=middle width=9.86687624999999pt height=14.15524440000002pt/> modes), use latent variable models or autoregressive discretization.


Imitation learning (at least behavioral cloning) is intrinsically limited and problematic: 

- Humans need to provide data, which is typically finite, but deep learning works best when data is plentiful.
- Humans are not good at providing some kinds of actions (e.g. when the task is overwhelming for a human).
- We don't meet an interesting feature of humans: being able to learn autonomously and to keep on improving with their self experience. 

That's why we would like to learn **without imitation**.

**Idea**: "what you really want is not to imitate somebody's actions, but to *minimize the expected number of times you're eaten by the tiger*" XD.


## Modifying terminology

<img src="/notes/Deep_RL_Berkeley/tex/b7193329124771544002dbfa26540c1f.svg?invert_in_darkmode&sanitize=true" align=middle width=207.03207689999996pt height=37.80850590000001pt/>


## Why Behavioral Cloning can be problematic

Tight rope walker.

### Basic derivation

Setting: 
- Length of the trajectory: <img src="/notes/Deep_RL_Berkeley/tex/2f118ee06d05f3c2d98361d9c30e38ce.svg?invert_in_darkmode&sanitize=true" align=middle width=11.889314249999991pt height=22.465723500000017pt/>. 
- Cost: <img src="/notes/Deep_RL_Berkeley/tex/23daaf0ddc7b6a7964b0d0cb17aff720.svg?invert_in_darkmode&sanitize=true" align=middle width=163.35609509999998pt height=24.65753399999998pt/> (doing a thing that the human would not have done).
- **Assumption**: <img src="/notes/Deep_RL_Berkeley/tex/34d26346163fccb1d1203ad87c1749f2.svg?invert_in_darkmode&sanitize=true" align=middle width=237.44072219999995pt height=24.65753399999998pt/> ((learning memorized human actions to a some degree).

> Let's construct the worst possible situation: 
>
> &rarr; The *imitation learning* tight rope walker.
>
> Imitation learning, since he is not afraid of falling because of a bad reward, but because he will end up in a state on which he has no training info (the human walker never fell).

<img src="/notes/Deep_RL_Berkeley/tex/062831ec6e16a1b51ee14ababe62cb9a.svg?invert_in_darkmode&sanitize=true" align=middle width=391.4987901pt height=37.80850590000001pt/>

Thus: <img src="/notes/Deep_RL_Berkeley/tex/f8e35e038d97e303ed2dda25bfdb4c6a.svg?invert_in_darkmode&sanitize=true" align=middle width=52.710132749999985pt height=26.76175259999998pt/>

Bad news: error scales quadratically with the length of the trajectory.

**BUT**: the assumption is not very realistic, since we can expect some generalization from the learning!

---

### More general derivation (used later to analyze Policy Gradient methods)

Let's take generalization into account in our new setting: 
- Same cost
- **Assumption**: <img src="/notes/Deep_RL_Berkeley/tex/a6b9c833cdd26b5ffb897f528630d97c.svg?invert_in_darkmode&sanitize=true" align=middle width=256.1701098pt height=24.65753399999998pt/>

>**Simple analysis**: with DAgger, <img src="/notes/Deep_RL_Berkeley/tex/d3ae1baf32d76d35eb299a1cfd0e6609.svg?invert_in_darkmode&sanitize=true" align=middle width=122.68309349999998pt height=24.65753399999998pt/>
>
>Therefore, all the encountered states with the policy are somehow in the training data. So, at each step, we have a <img src="/notes/Deep_RL_Berkeley/tex/9ae7733dac2b7b4470696ed36239b676.svg?invert_in_darkmode&sanitize=true" align=middle width=7.66550399999999pt height=14.15524440000002pt/> probability of error.
>
>So: <img src="/notes/Deep_RL_Berkeley/tex/ea4511b32571f21b139bf9d7028dcb1f.svg?invert_in_darkmode&sanitize=true" align=middle width=153.87355004999998pt height=37.80850590000001pt/> (best that we can do)

Now, for behavioral cloning, no such assumption (we have <img src="/notes/Deep_RL_Berkeley/tex/ebae61e7b8ce3b020d3f30d0dad500aa.svg?invert_in_darkmode&sanitize=true" align=middle width=77.22642674999999pt height=22.831056599999986pt/>).

<img src="/notes/Deep_RL_Berkeley/tex/7ecab46b1e4a553e526b756cbe59391f.svg?invert_in_darkmode&sanitize=true" align=middle width=384.3808254pt height=26.085962100000025pt/>

> Notation (*total variation divergence* between two distributions, here <img src="/notes/Deep_RL_Berkeley/tex/5f8e143b80227a85682626ace43e37cb.svg?invert_in_darkmode&sanitize=true" align=middle width=14.88586109999999pt height=14.15524440000002pt/> and <img src="/notes/Deep_RL_Berkeley/tex/b39b554716f49be3cdcfa71eca41cfdf.svg?invert_in_darkmode&sanitize=true" align=middle width=39.60108899999999pt height=14.15524440000002pt/>): 
> <img src="/notes/Deep_RL_Berkeley/tex/4b404e1a2ffcc8781df01fc4fcd5ab02.svg?invert_in_darkmode&sanitize=true" align=middle width=340.9053252pt height=24.657735299999988pt/>
> **Remark**: at most 2 (disjoint supports), at least zero (identical distributions).

We can express the difference between the training and the policy distributions, at instant t: 

<img src="/notes/Deep_RL_Berkeley/tex/9c43240d745106dc82926f908b774ee9.svg?invert_in_darkmode&sanitize=true" align=middle width=633.97613895pt height=27.94539330000001pt/>

For the second-to-last inequality, we used the fact that TVD is less than 2.

For the last inequality, we used: <img src="/notes/Deep_RL_Berkeley/tex/9b37f927d40171ce2271e34c80f4e7e6.svg?invert_in_darkmode&sanitize=true" align=middle width=195.45038039999997pt height=26.085962100000025pt/>

Now, let's get a bound for our cost:

<img src="/notes/Deep_RL_Berkeley/tex/31a1035b5cddd1051a88d221e3a4d9de.svg?invert_in_darkmode&sanitize=true" align=middle width=700.2741669pt height=64.11025499999998pt/>

So:
 
<img src="/notes/Deep_RL_Berkeley/tex/7fb80b376a0777f5a8d8c819a38b53bd.svg?invert_in_darkmode&sanitize=true" align=middle width=700.2741603pt height=57.53454959999999pt/>

(since <img src="/notes/Deep_RL_Berkeley/tex/f5a9df3928ee4b02c0f6c32afc70409d.svg?invert_in_darkmode&sanitize=true" align=middle width=67.82155049999999pt height=21.18721440000001pt/>). 

The first term is the expectation of error under the training distribution, so <img src="/notes/Deep_RL_Berkeley/tex/9ae7733dac2b7b4470696ed36239b676.svg?invert_in_darkmode&sanitize=true" align=middle width=7.66550399999999pt height=14.15524440000002pt/> by definition.

Therefore:

<img src="/notes/Deep_RL_Berkeley/tex/487047bbbc10b4908530a6e337a0ad2d.svg?invert_in_darkmode&sanitize=true" align=middle width=292.90788614999997pt height=37.80850590000001pt/>

**&rarr; Again, we find a <img src="/notes/Deep_RL_Berkeley/tex/f8e35e038d97e303ed2dda25bfdb4c6a.svg?invert_in_darkmode&sanitize=true" align=middle width=52.710132749999985pt height=26.76175259999998pt/> bound.**